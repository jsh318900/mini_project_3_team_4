{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66f5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2, glob, os, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from albumentations.augmentations.geometric.transforms import PadIfNeeded\n",
    "from torchvision.models import efficientnet_b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a1387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "root_dir = './datasets/'\n",
    "labelencoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75fc337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 정리\n",
    "train_label_frame = pd.read_csv(root_dir + 'train.csv', index_col=1)\n",
    "test_index_file_frame = pd.read_csv(root_dir + 'test.csv', index_col=1)\n",
    "artist_info_frame = pd.read_csv(root_dir + 'artists_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446ea4c",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71ac2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtworkTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, file_label_frame):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_label_frame = file_label_frame\n",
    "        if self.file_label_frame['artist'].dtype == np.object0:\n",
    "            raise TypeError('라벨값을 정수로 인코딩한 후에 적제해주세요.')\n",
    "        self.padder = PadIfNeeded(min_height=None,\n",
    "                                  min_width=None,\n",
    "                                  pad_height_divisor=10,\n",
    "                                  pad_width_divisor=10,\n",
    "                                  border_mode=cv2.BORDER_CONSTANT\n",
    "                                 )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_label_frame.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.file_label_frame.index[idx]\n",
    "        image_label = self.file_label_frame.loc[image_path, 'artist']\n",
    "        image = cv2.imread(self.root_dir + image_path)\n",
    "        padded = self.padder(image=image)['image']\n",
    "        transform = A.Compose([A.Resize(224,224), A.Normalize(), ToTensorV2()])\n",
    "        \n",
    "        w = padded.shape[0]\n",
    "        h = padded.shape[1]\n",
    "        # layer2\n",
    "        crops_layer2 =torch.cat([\n",
    "                               transform(image=padded[:w//2, :h//2, :])['image'],\n",
    "                               transform(image=padded[:w//2, h//2:, :])['image'],\n",
    "                               transform(image=padded[w//2:, :h//2, :])['image'],\n",
    "                               transform(image=padded[w//2:, h//2:, :])['image']\n",
    "        ], dim=0)\n",
    "        crops_layer3 = []\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                start_w, start_h = i * (w // 5), j * (h // 5)\n",
    "                end_w, end_h = (i + 1) * (w // 5), (j + 1) * (h // 5)\n",
    "                crops_layer3.append(transform(image=padded[start_w:end_w, start_h:end_h, :])['image'])\n",
    "        crops_layer3 = torch.cat(crops_layer3, dim=0)\n",
    "        \n",
    "        image = torch.Tensor(transform(image=padded)['image'])\n",
    "        return torch.cat([image, crops_layer2, crops_layer3], dim=0).reshape(30, 3, 224, 224), image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6bfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_frame['artist'] = labelencoder.fit_transform(train_label_frame['artist'])\n",
    "train = ArtworkTrainDataset(root_dir, train_label_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf711a1",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7352d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidNet(nn.Module):\n",
    "    def __init__(self, baseline, baseline_kwargs, num_baseline_output, num_classes):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            baseline(**baseline_kwargs),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=num_baseline_output, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x[0].size()[0]\n",
    "        rest = x[0].size()[1:3]\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        x = self.network(x)\n",
    "        layers = torch.split(x, split_size_or_sections=[1, 4, 25])\n",
    "        \n",
    "        return softmax(25 * layers[0].flatten()) + 4 * softmax(layers[1]).mean(dim=0) + softmax(layers[2]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b869b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyramidNet(efficientnet_b0, {'weights':torchvision.models.EfficientNet_B0_Weights.DEFAULT}, 1000, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec9614",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde26187",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150e1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(range(len(train)), test_size=0.2, random_state=12345, stratify=train_label_frame['artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83448c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Subset(train, train_idx)\n",
    "validset = Subset(train, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcefc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in trainset:\n",
    "        pred = model(data)\n",
    "        label_t = torch.zeros(50)\n",
    "        label_t[label] = 1\n",
    "        loss = loss_fn(pred, label_t)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    for data, label in validset:\n",
    "        pred = model(data)\n",
    "        label_t = torch.zeros(50)\n",
    "        label_t[label] = 1\n",
    "        loss = loss_fn(pred, label_t)\n",
    "        valid_loss += loss.item()\n",
    "    train_loss /= len(trainset)\n",
    "    valid_loss /= len(validset)\n",
    "    scheduler.step(valid_loss)\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
